<!DOCTYPE html>

<html>
  <head>
    <title></title>
    <style media="screen">
      section { 
        display: inline-block;
        width: 45%;
        vertical-align: top;
      }
    
      .document {
        max-width: 20em;
        background: #dff;
        margin-top: 1em;
      }
      .document h2 {
        margin: 0;
      }
      .document:after {
        content: "Tags: " attr(data-tags);
        display: block;
        font-family: sans-serif;
        color: #555;
      }
    </style>
  </head>
  <body>
    <section class="search">
      <h1>Search</h1>
      
      Search: <input type="text" name="query" value="" placeholder="your search query">
      <br>
      Filter tags: <input type="text" name="tags" value="" placeholder="foo, bar, etc.">
      
      <ul id="search-results">
        
      </ul>
      
      <h1>Code</h1>
      <pre><code>var si = require('../')({indexPath: 'si-example'});
        
// grab "documents" from the DOM.
var docs = Array.prototype.map.call(document.querySelectorAll('.document'),
function(doc) {
  return {
    'id': doc.id,
    'title': doc.querySelector('h2').textContent,
    'body': doc.querySelector('p').textContent,
    'tags': doc.getAttribute('data-tags').split(/\s|,/)
  }
});

// index them
si.add({ batchName: 'initial', filters: ['tags'] }, docs, function(err) {
  if (err) throw err;
  console.log('indexed!');
})

var queryElement = document.querySelector('[name="query"]');
var tagsElement = document.querySelector('[name="tags"]')
var timeout;
[queryElement, tagsElement].forEach(function(el) {
  el.addEventListener('keyup', function() {
    if(timeout) clearTimeout(timeout);
    timeout = setTimeout(search,500)
  });
})

function search() {
  var query = {
    "query": {
      "*": queryElement.value.split(/[^\w]+/)
    },
    "offset": "0",
    "pageSize": "20",
    "weight": {
      "title": ["10"]
    }
  };
  var filter = tagsElement.value.trim();
  if(filter.length > 0) query.filter = { "tags": filter.split(/[^\w-]+/) }
  
  si.search(query, function(err, results) {
    results = results.hits.sort(function(a, b) {
      return b.score - a.score;
    });
    var summary = results.map(function(res) {
      return res.document.title + ': search score='+res.score;
    });
    document.querySelector('#search-results').innerHTML = summary.map(
    function(item) { return '<li>' + item + '</li>'}
    ).join('');
  })
}
      </code></pre>
    </section>
    
    <section class="documents">
      <h1>"Documents":</h1>
      <div class="document" data-tags="foo bar" id="doc-1">
        <h2>A really interesting document.</h2>
        <p>This is a really interesting document</p>
      </div>
      <div class="document" data-tags="bar baz" id="doc-2">
        <h2>Another interesting document</h2>
        <p>This is another really interesting document that is a bit different</p>
      </div>
      <div class="document" data-tags="readme" id="doc-3">
        <h2>README for search-index module.</h2>
        <p>
          Table of Contents generated with DocToc
          
          Search-index
          The Norch Search Engine
          Features
          Installation
          Usage
          Initialization
          API
          si.add
          si.del
          si.get
          si.search
          si.match
          si.tellMeAboutMySearchIndex
          si.empty
          si.snapshot
          si.replicate
          Query Parameters
          query
          offset
          pageSize
          facets
          facetLength
          facetSort
          teaser
          weight
          filter
          License
          Search-index
          NPM
          
          NPM
          
          http://npm-stat.vorba.ch/charts.html?package=search-index
          
          Search-index is a text search module for Node.js. Think "node version of Lucene, but much simpler".
          
          Search-index allows you to perform free-text search over structured or unstructured data, and return a resultset ordered by relevance.
          
          Search-index is built with the soooperfast levelUP module, and the very useful Natural module.
          
          The Point of Search-Index is to simplify set up and operation of an search engine. Search-index is essentially free from configuration- the index is dynamic and morphs into the structure that you require automatically, based on the documents that it is fed.
          
          Search-index is in an alpha stage- meaning that it has been known to work quite well, but edge cases and portability may be challenging. Query-result is robust. See known issues and performance tips below.
          
          The Norch Search Engine
          Search-index is currently the index powering the Norch search engine.
          
          Features
          Full text search
          Stopword removal
          Faceting
          Filtering
          Fielded search
          Field weighting
          Relevance weighting (tf-idf)
          Paging (offset and resultset length)
          Teasers
          Installation
          Releases are listed here. Generally you will want the most recent one.
          
          The easiest way to include search-index in your project is by using npm
          
          npm install search-index
          The module can then be initialised by including the line
          
          si = require('search-index');
          at the top of your app.
          
          Usage
          To make a searchable index, you must first add documents with si.add.
          
          Documents are then searchable with si.search.
          
          Initialization
          search-index is called with require like so:
          
          var si = require('search-index');
          search-index can be initialized with options like so:
          
          var options = { indexPath: 'si2', logLevel: 'error', logSilent: false }
          var si = require('search-index')(options)
          Available options
          
          indexPath The physical location of the index on the filesystem. Default is si
          logLevel A winston log level like info, debug (lots of logs) or error (nearly silent). Default is info
          logSilent Set to true to suppress any log the search-index might do
          API
          si.add
          
          Inserts document into the index
          
          si.add({'batchName': batchName, 'filters': filters}, data, function(err) {
            if (!err) console.log('indexed!');
          });
          Where batchName is any name to tag the batch, and filters tells the index which fields can be filtered and agregated on, and data is an object containing one or more documents in a format similar to:
          
          [
          {
            'id':'1'
            'title':'A really interesting document',
            'body':'This is a really interesting document',
            'metadata':['red', 'potato']
          },
          {
            'id':'2'
            'title':'Another interesting document',
            'body':'This is another really interesting document that is a bit different',
            'metadata':['yellow', 'potato']
          }
          ]
          ...and filters is an array of field names that may be contained in the document that the index will use for building filters. A filter field must always be an array of single String tokens, for example ['metadata','places']. 'search-index' wont accept strings, so remember to wrap home-rolled JSON with 'JSON.parse' to turn it into an object.
          
          Example:
          
          var batch = [
          {
            'id':'1'
            'title':'A really interesting document',
            'body':'This is a really interesting document',
            'metadata':['red', 'potato']
          },
          {
            'id':'2'
            'title':'Another interesting document',
            'body':'This is another really interesting document that is a bit different',
            'metadata':['yellow', 'potato']
          }
          ];
          var batchName = 'twoDocs';
          var filters = ['metadata'];
          
          si.add({'batchName': batchName, 'filters': filters}, data, function(err) {
            if (!err) console.log('indexed!');
          });
          Note: if you dont specify an id field, search-index will specify one for you.
          
          si.del
          
          Delete the document and all associated index entries.
          
          si.del(docID, function(err) {
            if (!err) console.log('success!');
          });
          si.get
          
          Get the document and all associated index entries.
          
          si.get(docID, function(err, doc) {
            if (!err) console.log(doc);
          });
          si.search
          
          Queries the search index
          
          si.search(query, function(err, results) {
            //check for errors and do something with search results, for example this:
            if (!err) console.log(results)
          });
          ...where query is an object similar to (see Query Parameters for more info):
          
          {
            "query": {
              "*": ["usa"]
            },
            "offset": "0",
            "pageSize": "20",
            "facets": [
            "places",
            "organisations"
            ],
            "weight": {
              "title": [
              "10"
              ]
            },
            "filter": {
              "places": [
              "usa"
              ]
            }
          }
          si.match
          
          A matcher is a service that generates a dictionary of words based on the contents of the index, and then returns appropriate sets based on substrings. For example, once the matcher is generated, a beginsWith of "lon" might return ['London', 'longing', 'longitude'] depending on the contents of the index. Terms are ordered by total occurances in index.
          
          si.matcher(beginsWith, function(err, matches) {
            if (!err) console.log(matches);
          });
          si.tellMeAboutMySearchIndex
          
          Returns metadata about the state of the index.
          
          si.tellMeAboutMySearchIndex(function(msg) {
            console.log(msg);
          });
          si.empty
          
          Empties the search index, can be used in conjunction with replication.
          
          si.empty(function(err) {
            if (!err) console.log("Emptied! Search-index now contains no documents- please refeed or replicate");
          });
          si.snapshot
          
          Returns a readStream that can then be piped on, for instance to file.
          
          //assumes that: var fs = require('fs')
          si.snapShot(function(readStream) {
            readStream.pipe(fs.createWriteStream('backup.gz'))
            .on('close', function() {
              //a snapshot of the search-index now exists in the file 'backup.gz'
            });
          });
          si.replicate
          
          Replicates an index from a snapshot file generated by si.snapshot.
          
          //assumes that backup is in a file called 'backup.gz'
          si.replicate(fs.createReadStream('backup.gz'), function(msg){
            that.completed = true;
          });
          Query Parameters
          query
          
          A free text string containing one or many tokens. * is equivalent to 'search all fields'
          
          "query": {"*":["usa"]}
          You can also specify named fields like so :
          
          "query": {
            "title":["usa", "reagan"]
          }
          note: currently you can only limit to one field
          
          offset
          
          The starting point in the resultset of the results returned. Useful for paging
          
          "offset": 0
          pageSize
          
          The maximum number of results in the resultset that can be returned. Counted from offset
          
          "pageSize": 20
          facets
          
          Allows faceted navigation, the parameter is an array of fields. If no facets is sent, all possible facets are returned.
          
          "facets": [
          "topics",
          "places"
          ]
          facetLength
          
          Defines the amount of entries per facet category. Defaults to 10.
          
          "facets": [
          "topics",
          "places"
          ],
          "facetLength": 50
          facetSort
          
          Defines the sort order of facets. Facets can be sorted on keys or values in an ascending or descening order. Possible values for facetSort are keyAsc, keyDesc, valueAsc, and valueDesc. The default sort is equivalent to valueDesc.
          
          "facets": [
          "topics",
          "places"
          ],
          "facetLength": 50,
          "facetSort": "keyAsc"
          teaser
          
          Creates a field that shows where the search terms exist in the given field. For example, a teaser field could be generated from the document field body
          
          "teaser": "body"
          weight
          
          Sets a factor by which the score of a field should be weighted. Useful for building custom relevancy models
          
          "weight": {
            "body": [
            "10"
            ],
            "title": [
            "5"
            ]
          }
          filter
          
          Used to return a resultset on a facet.
          
          "filter": {
            "places": [
            "ussr"
            ]
          }
          License
          Search-index is released under the MIT license:
          
          Copyright (c) 2013 Fergus McDowall
          
          Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
          
          The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
          
          THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
        </p>
      </div>
      <div class="document" data-tags="readme">
        <h2>README for 'natural' package.</h2>
        <p>
          natural
          Build Status
          
          "Natural" is a general natural language facility for nodejs. Tokenizing, stemming, classification, phonetics, tf-idf, WordNet, string similarity, and some inflections are currently supported.
          
          It's still in the early stages, so we're very interested in bug reports, contributions and the like.
          
          Note that many algorithms from Rob Ellis's node-nltools are being merged into this project and will be maintained from here onward.
          
          At the moment, most of the algorithms are English-specific, but in the long-term, some diversity will be in order. Thanks to Polyakov Vladimir, Russian stemming has been added!, Thanks to David Przybilla, Spanish stemming has been added!.
          
          Aside from this README, the only documentation is this DZone article and here on my blog, which is a bit older.
          
          Installation
          
          If you're just looking to use natural without your own node application, you can install via NPM like so:
          
          npm install natural
          If you're interested in contributing to natural, or just hacking on it, then by all means fork away!
          
          Tokenizers
          
          Word, Regexp, and Treebank tokenizers are provided for breaking text up into arrays of tokens:
          
          var natural = require('natural'),
          tokenizer = new natural.WordTokenizer();
          console.log(tokenizer.tokenize("your dog has fleas."));
          // [ 'your', 'dog', 'has', 'fleas' ]
          The other tokenizers follow a similar pattern:
          
          tokenizer = new natural.TreebankWordTokenizer();
          console.log(tokenizer.tokenize("my dog hasn't any fleas."));
          // [ 'my', 'dog', 'has', 'n\'t', 'any', 'fleas', '.' ]
          
          tokenizer = new natural.RegexpTokenizer({pattern: /\-/});
          console.log(tokenizer.tokenize("flea-dog"));
          // [ 'flea', 'dog' ]
          
          tokenizer = new natural.WordPunctTokenizer();
          console.log(tokenizer.tokenize("my dog hasn't any fleas."));
          // [ 'my',  'dog',  'hasn',  '\'',  't',  'any',  'fleas',  '.' ]
          String Distance
          
          Natural provides an implementation of the Jaro–Winkler string distance measuring algorithm. This will return a number between 0 and 1 which tells how closely the strings match (0 = not at all, 1 = exact match):
          
          var natural = require('natural');
          console.log(natural.JaroWinklerDistance("dixon","dicksonx"))
          console.log(natural.JaroWinklerDistance('not', 'same'));
          Output:
          
          0.7466666666666666
          0
          Natural also offers support for Levenshtein distances:
          
          var natural = require('natural');
          console.log(natural.LevenshteinDistance("ones","onez"));
          console.log(natural.LevenshteinDistance('one', 'one'));
          Output:
          
          1
          0
          The cost of the three edit operations are modifiable for Levenshtein:
          
          console.log(natural.LevenshteinDistance("ones","onez", {
            insertion_cost: 1,
            deletion_cost: 1,
            substitution_cost: 1
          }));
          Output:
          
          1
          And Dice's co-efficient:
          
          var natural = require('natural');
          console.log(natural.DiceCoefficient('thing', 'thing'));
          console.log(natural.DiceCoefficient('not', 'same'));
          Output:
          
          1
          0
          Stemmers
          
          Currently stemming is supported via the Porter and Lancaster (Paice/Husk) algorithms.
          
          var natural = require('natural');
          This example uses a Porter stemmer. "word" is returned.
          
          console.log(natural.PorterStemmer.stem("words")); // stem a single word
          in Russian:
          
          console.log(natural.PorterStemmerRu.stem("падший"));
          in Spanish:
          
          console.log(natural.PorterStemmerEs.stem("jugaría"));
          attach() patches stem() and tokenizeAndStem() to String as a shortcut to PorterStemmer.stem(token). tokenizeAndStem() breaks text up into single words and returns an array of stemmed tokens.
          
          natural.PorterStemmer.attach();
          console.log("i am waking up to the sounds of chainsaws".tokenizeAndStem());
          console.log("chainsaws".stem());
          the same thing can be done with a Lancaster stemmer:
          
          natural.LancasterStemmer.attach();
          console.log("i am waking up to the sounds of chainsaws".tokenizeAndStem());
          console.log("chainsaws".stem());
          Classifiers
          
          Two classifiers are currently supported, Naive Bayes and logistic regression. The following examples use the BayesClassifier class, but the LogisticRegressionClassifier class could be substituted instead.
          
          var natural = require('natural'),
          classifier = new natural.BayesClassifier();
          You can train the classifier on sample text. It will use reasonable defaults to tokenize and stem the text.
          
          classifier.addDocument('i am long qqqq', 'buy');
          classifier.addDocument('buy the q''s', 'buy');
          classifier.addDocument('short gold', 'sell');
          classifier.addDocument('sell gold', 'sell');
          
          classifier.train();
          Outputs "sell"
          
          console.log(classifier.classify('i am short silver'));
          Outputs "buy"
          
          console.log(classifier.classify('i am long copper'));
          You have access to the set of matched classes and the associated value from the classifier.
          
          Outputs:
          
          [ { label: 'sell', value: 0.39999999999999997 },
          { label: 'buy', value: 0.19999999999999998 } ]
          From this:
          
          console.log(classifier.getClassifications('i am long copper'));
          The classifier can also be trained with and can classify arrays of tokens, strings, or any mixture of the two. Arrays let you use entirely custom data with your own tokenization/stemming, if you choose to implement it.
          
          classifier.addDocument(['sell', 'gold'], 'sell');
          The training process can be monitored by subscribing to the event trainedWithDocument that's emitted by the classifier, this event's emitted each time a document is finished being trained against:
          
          classifier.events.on('trainedWithDocument', function (obj) {
            console.log(obj);
            /* {
              *   total: 23 // There are 23 total documents being trained against
              *   index: 12 // The index/number of the document that's just been trained against
              *   doc: {...} // The document that has just been indexed
              */ }
            });
            A classifier can also be persisted and recalled so you can reuse a training
            
            classifier.save('classifier.json', function(err, classifier) {
              // the classifier is saved to the classifier.json file!
            });
            To recall from the classifier.json saved above:
            
            natural.BayesClassifier.load('classifier.json', null, function(err, classifier) {
              console.log(classifier.classify('long SUNW'));
              console.log(classifier.classify('short SUNW'));
            });
            A classifier can also be serialized and deserialized like so:
            
            var classifier = new natural.BayesClassifier();
            classifier.addDocument(['sell', 'gold'], 'sell');
            classifier.addDocument(['buy', 'silver'], 'buy');
            
            // serialize
            var raw = JSON.stringify(classifier);
            // deserialize
            var restoredClassifier = natural.BayesClassifier.restore(JSON.parse(raw));
            console.log(restoredClassifier.classify('i should sell that'));
            Phonetics
            
            Phonetic matching (sounds-like) matching can be done with the SoundEx, Metaphone or DoubleMetaphone algorithms
            
            var natural = require('natural'),
            metaphone = natural.Metaphone, soundEx = natural.SoundEx;
            
            var wordA = 'phonetics';
            var wordB = 'fonetix';
            To test the two words to see if they sound alike:
            
            if(metaphone.compare(wordA, wordB))
            console.log('they sound alike!');
            The raw phonetics are obtained with process():
            
            console.log(metaphone.process('phonetics'));
            A maximum code length can be supplied:
            
            console.log(metaphone.process('phonetics', 3));
            DoubleMetaphone deals with two encodings returned in an array. This feature is experimental and subject to change:
            
            var natural = require('natural'),
            dm = natural.DoubleMetaphone;
            
            var encodings = dm.process('Matrix');
            console.log(encodings[0]);
            console.log(encodings[1]);
            Attaching will patch String with useful methods:
            
            metaphone.attach();
            soundsLike is essentially a shortcut to Metaphone.compare:
            
            if(wordA.soundsLike(wordB))
            console.log('they sound alike!');
            The raw phonetics are obtained with phonetics():
            
            console.log('phonetics'.phonetics());
            Full text strings can be tokenized into arrays of phonetics (much like how tokenization-to-arrays works for stemmers):
            
            console.log('phonetics rock'.tokenizeAndPhoneticize());
            Same module operations applied with SoundEx:
            
            if(soundEx.compare(wordA, wordB))
            console.log('they sound alike!');
            The same String patches apply with soundEx:
            
            soundEx.attach();
            
            if(wordA.soundsLike(wordB))
            console.log('they sound alike!');
            
            console.log('phonetics'.phonetics());
            Inflectors
            
            Nouns
            
            Nouns can be pluralized/singularized with a NounInflector:
            
            var natural = require('natural'),
            nounInflector = new natural.NounInflector();
            To pluralize a word (outputs "radii"):
            
            console.log(nounInflector.pluralize('radius'));
            To singularize a word (outputs "beer"):
            
            console.log(nounInflector.singularize('beers'));
            Like many of the other features, String can be patched to perform the operations directly. The "Noun" suffix on the methods is necessary, as verbs will be supported in the future.
            
            nounInflector.attach();
            console.log('radius'.pluralizeNoun());
            console.log('beers'.singularizeNoun());
            Numbers
            
            Numbers can be counted with a CountInflector:
            
            var countInflector = natural.CountInflector;
            Outputs "1st":
            
            console.log(countInflector.nth(1));
            Outputs "111th":
            
            console.log(countInflector.nth(111));
            Present Tense Verbs
            
            Present Tense Verbs can be pluralized/singularized with a PresentVerbInflector. This feature is still experimental as of 0.0.42, so use with caution, and please provide feedback.
            
            var verbInflector = new natural.PresentVerbInflector();
            Outputs "becomes":
            
            console.log(verbInflector.singularize('become'));
            Outputs "become":
            
            console.log(verbInflector.pluralize('becomes'));
            Like many other natural modules, attach() can be used to patch strings with handy methods.
            
            verbInflector.attach();
            console.log('walk'.singularizePresentVerb());
            console.log('walks'.pluralizePresentVerb());
            N-Grams
            
            n-grams can be obtained for either arrays or strings (which will be tokenized for you):
            
            var NGrams = natural.NGrams;
            bigrams
            
            console.log(NGrams.bigrams('some words here'));
            console.log(NGrams.bigrams(['some',  'words',  'here']));
            Both of the above output: [ [ 'some', 'words' ], [ 'words', 'here' ] ]
            
            trigrams
            
            console.log(NGrams.trigrams('some other words here'));
            console.log(NGrams.trigrams(['some',  'other', 'words',  'here']));
            Both of the above output: [ [ 'some', 'other', 'words' ], [ 'other', 'words', 'here' ] ]
            
            arbitrary n-grams
            
            console.log(NGrams.ngrams('some other words here for you', 4));
            console.log(NGrams.ngrams(['some', 'other', 'words', 'here', 'for',
            'you'], 4));
            The above outputs: [ [ 'some', 'other', 'words', 'here' ], [ 'other', 'words', 'here', 'for' ], [ 'words', 'here', 'for', 'you' ] ]
            
            padding
            
            n-grams can also be returned with left or right padding by passing a start and/or end symbol to the bigrams, trigrams or ngrams.
            
            console.log(NGrams.ngrams('some other words here for you', 4, '[start]', '[end]'));
            The above will output:
            
            [ [ '[start]', '[start]', '[start]', 'some' ],
            [ '[start]', '[start]', 'some', 'other' ],
            [ '[start]', 'some', 'other', 'words' ],
            [ 'some', 'other', 'words', 'here' ],
            [ 'other', 'words', 'here', 'for' ],
            [ 'words', 'here', 'for', 'you' ],
            [ 'here', 'for', 'you', '[end]' ],
            [ 'for', 'you', '[end]', '[end]' ],
            [ 'you', '[end]', '[end]', '[end]' ] ]
            For only end symbols, pass null for the start symbol, for instance:
            
            console.log(NGrams.ngrams('some other words here for you', 4, null, '[end]'));
            Will output:
            
            [ [ 'some', 'other', 'words', 'here' ],
            [ 'other', 'words', 'here', 'for' ],
            [ 'words', 'here', 'for', 'you' ],
            [ 'here', 'for', 'you', '[end]' ],
            [ 'for', 'you', '[end]', '[end]' ],
            [ 'you', '[end]', '[end]', '[end]' ] ]
            NGramsZH
            
            For Chinese like languages, you can use NGramsZH to do a n-gram, and all apis are the same:
            
            var NGramsZH = natural.NGramsZH;
            console.log(NGramsZH.bigrams('中文测试'));
            console.log(NGramsZH.bigrams(['中',  '文',  '测', '试']));
            console.log(NGramsZH.trigrams('中文测试'));
            console.log(NGramsZH.trigrams(['中',  '文', '测',  '试']));
            console.log(NGramsZH.ngrams('一个中文测试', 4));
            console.log(NGramsZH.ngrams(['一', '个', '中', '文', '测',
            '试'], 4));
            tf-idf
            
            Term Frequency–Inverse Document Frequency (tf-idf) is implemented to determine how important a word (or words) is to a document relative to a corpus. The following example will add four documents to a corpus and determine the weight of the word "node" and then the weight of the word "ruby" in each document.
            
            var natural = require('natural'),
            TfIdf = natural.TfIdf,
            tfidf = new TfIdf();
            
            tfidf.addDocument('this document is about node.');
            tfidf.addDocument('this document is about ruby.');
            tfidf.addDocument('this document is about ruby and node.');
            tfidf.addDocument('this document is about node. it has node examples');
            
            console.log('node --------------------------------');
            tfidf.tfidfs('node', function(i, measure) {
              console.log('document #' + i + ' is ' + measure);
            });
            
            console.log('ruby --------------------------------');
            tfidf.tfidfs('ruby', function(i, measure) {
              console.log('document #' + i + ' is ' + measure);
            });
            The above outputs:
            
            node --------------------------------
            document #0 is 1.4469189829363254
            document #1 is 0
            document #2 is 1.4469189829363254
            document #3 is 2.8938379658726507
            ruby --------------------------------
            document #0 is 0
            document #1 is 1.466337068793427
            document #2 is 1.466337068793427
            document #3 is 0
            This approach can also be applied to individual documents.
            
            The following example measures the term "node" in the first and second documents.
            
            console.log(tfidf.tfidf('node', 0));
            console.log(tfidf.tfidf('node', 1));
            A TfIdf instance can also load documents from files on disk.
            
            var tfidf = new TfIdf();
            tfidf.addFileSync('data_files/one.txt');
            tfidf.addFileSync('data_files/two.txt');
            Multiple terms can be measured as well, with their weights being added into a single measure value. The following example determines that the last document is the most relevant to the words "node" and "ruby".
            
            var natural = require('natural'),
            TfIdf = natural.TfIdf,
            tfidf = new TfIdf();
            
            tfidf.addDocument('this document is about node.');
            tfidf.addDocument('this document is about ruby.');
            tfidf.addDocument('this document is about ruby and node.');
            
            tfidf.tfidfs('node ruby', function(i, measure) {
              console.log('document #' + i + ' is ' + measure);
            });
            The above outputs:
            
            document #0 is 1.2039728043259361
            document #1 is 1.2039728043259361
            document #2 is 2.4079456086518722
            The examples above all use strings, which case natural to automatically tokenize the input. If you wish to perform your own tokenization or other kinds of processing, you can do so, then pass in the resultant arrays later. This approach allows you to bypass natural's default preprocessing.
            
            var natural = require('natural'),
            TfIdf = natural.TfIdf,
            tfidf = new TfIdf();
            
            tfidf.addDocument(['document', 'about', 'node']);
            tfidf.addDocument(['document', 'about', 'ruby']);
            tfidf.addDocument(['document', 'about', 'ruby', 'node']);
            tfidf.addDocument(['document', 'about', 'node', 'node', 'examples']);
            
            tfidf.tfidfs(['node', 'ruby'], function(i, measure) {
              console.log('document #' + i + ' is ' + measure);
            });
            It's possible to retrieve a list of all terms in a document, sorted by their importance.
            
            tfidf.listTerms(0 /*document index*/).forEach(function(item) {
              console.log(item.term + ': ' + item.tfidf);
            });
            A TfIdf instance can also be serialized and deserialized for save and recall.
            
            var tfidf = new TfIdf();
            tfidf.addDocument('document one', 'un');
            tfidf.addDocument('document Two', 'deux');
            var s = JSON.stringify(tfidf);
            // save "s" to disk, database or otherwise
            
            // assuming you pulled "s" back out of storage.
            var tfidf = new TfIdf(JSON.parse(s));
            Tries
            
            Tries are a very efficient data structure used for prefix-based searches. Natural comes packaged with a basic Trie implementation which can support match collection along a path, existence search and prefix search.
            
            Building The Trie
            
            You need to add words to build up the dictionary of the Trie, this is an example of basic Trie set up:
            
            var natural = require('natural'),
            Trie = natural.Trie;
            
            var trie = new Trie();
            
            // Add one string at a time
            trie.addString("test");
            
            // Or add many strings
            trie.addStrings(["string1", "string2", "string3"]);
            Searching
            
            Contains
            
            The most basic operation on a Trie is to see if a search string is marked as a word in the Trie.
            
            console.log(trie.contains("test")); // true
            console.log(trie.contains("asdf")); // false
            Find Prefix
            
            The find prefix search will find the longest prefix that is identified as a word in the trie. It will also return the remaining portion of the string which it was not able to match.
            
            console.log(trie.findPrefix("tester"));     // ['test', 'er']
            console.log(trie.findPrefix("string4"));    // [null, '4']
            console.log(trie.findPrefix("string3"));    // ['string3', '']
            All Prefixes on Path
            
            This search will return all prefix matches along the search string path.
            
            trie.addString("tes");
            trie.addString("est");
            console.log(trie.findMatchesOnPath("tester")); // ['tes', 'test'];
            All Keys with Prefix
            
            This search will return all of the words in the Trie with the given prefix, or [ ] if not found.
            
            console.log(trie.keysWithPrefix("string")); // ["string1", "string2", "string3"]
            Case-Sensitivity
            
            By default the trie is case-sensitive, you can use it in case-_in_sensitive mode by passing false to the Trie constructor.
            
            trie.contains("TEST"); // false
            
            var ciTrie = new Trie(false);
            ciTrie.addString("test");
            ciTrie.contains("TEsT"); // true
            In the case of the searches which return strings, all strings returned will be in lower case if you are in case-_in_sensitive mode.
            
            EdgeWeightedDigraph
            
            EdgeWeightedDigraph represents a digraph, you can add an edge, get the number vertexes, edges, get all edges and use toString to print the Digraph.
            
            initialize a digraph:
            
            var EdgeWeightedDigraph = natural.EdgeWeightedDigraph;
            var digraph = new EdgeWeightedDigraph();
            digraph.add(5,4,0.35);
            digraph.add(5,1,0.32);
            digraph.add(1,3,0.29);
            digraph.add(6,2,0.40);
            digraph.add(3,6,0.52);
            digraph.add(6,4,0.93);
            the api used is: add(from, to, weight).
            
            get the number of vertexes:
            
            console.log(digraph.v());
            you will get 5.
            
            get the number of edges:
            
            console.log(digraph.e());
            you will get 5.
            
            ShortestPathTree
            
            ShortestPathTree represents a data type for solving the single-source shortest paths problem in edge-weighted directed acyclic graphs (DAGs). The edge weights can be positive, negative, or zero. There are three APIs: getDistTo(vertex), hasPathTo(vertex), pathTo(vertex).
            
            var ShortestPathTree = natural.ShortestPathTree;
            var spt = new ShortestPathTree(digraph, 5);
            digraph is an instance of EdgeWeightedDigraph, the second param is the start vertex of DAG.
            
            getDistTo(vertex)
            
            Will return the dist to vertex.
            
            console.log(spt.getDistTo(4));
            the output will be: 0.35
            
            hasDistTo(vertex)
            
            console.log(spt.hasDistTo(4));
            console.log(spt.hasDistTo(5));
            output will be:
            
            true
            false
            pathTo(vertex)
            
            this will return a shortest path:
            
            console.log(spt.pathTo(4));
            output will be:
            
            [5, 4]
            LongestPathTree
            
            LongestPathTree represents a data type for solving the single-source shortest paths problem in edge-weighted directed acyclic graphs (DAGs). The edge weights can be positive, negative, or zero. There are three APIs same as ShortestPathTree: getDistTo(vertex), hasPathTo(vertex), pathTo(vertex).
            
            var ShortestPathTree = natural.ShortestPathTree;
            var spt = new ShortestPathTree(digraph, 5);
            digraph is an instance of EdgeWeightedDigraph, the second param is the start vertex of DAG.
            
            getDistTo(vertex)
            
            Will return the dist to vertex.
            
            console.log(spt.getDistTo(4));
            the output will be: 2.06
            
            hasDistTo(vertex)
            
            console.log(spt.hasDistTo(4));
            console.log(spt.hasDistTo(5));
            output will be:
            
            true
            false
            pathTo(vertex)
            
            this will return a shortest path:
            
            console.log(spt.pathTo(4));
            output will be:
            
            [5, 1, 3, 6, 4]
            WordNet
            
            One of the newest and most experimental features in natural is WordNet integration. Here's an example of using natural to look up definitions of the word node. To use the WordNet module, first install the WordNet database files using the WNdb module:
            
            npm install WNdb
            (For node &lt; v0.6, please use 'npm install WNdb@3.0.0')
            
            Keep in mind that the WordNet integration is to be considered experimental at this point, and not production-ready. The API is also subject to change.
            
            Here's an example of looking up definitions for the word, "node".
            
            var wordnet = new natural.WordNet();
            
            wordnet.lookup('node', function(results) {
              results.forEach(function(result) {
                console.log('------------------------------------');
                console.log(result.synsetOffset);
                console.log(result.pos);
                console.log(result.lemma);
                console.log(result.synonyms);
                console.log(result.pos);
                console.log(result.gloss);
              });
            });
            Given a synset offset and a part of speech, a definition can be looked up directly.
            
            var wordnet = new natural.WordNet();
            
            wordnet.get(4424418, 'n', function(result) {
              console.log('------------------------------------');
              console.log(result.lemma);
              console.log(result.pos);
              console.log(result.gloss);
              console.log(result.synonyms);
            });
            If you have manually downloaded the WordNet database files, you can pass the folder to the constructor:
            
            var wordnet = new natural.WordNet('/my/wordnet/dict');
            As of v0.1.11, WordNet data files are no longer automatically downloaded.
            
            Princeton University "About WordNet." WordNet. Princeton University. 2010. http://wordnet.princeton.edu
            
            Spellcheck
            
            A probabilistic spellchecker based on http://norvig.com/spell-correct.html
            
            This is best constructed with an array of tokens from a corpus, but a simple list of words from a dictionary will work.
            
            var corpus = ['something', 'soothing'];
            var spellcheck = new Spellcheck(corpus);
            It uses the trie datastructure for fast boolean lookup of a word
            
            spellcheck.isCorrect('cat'); // false
            It suggests corrections (sorted by probability in descending order) that are up to a maximum edit distance away from the input word. According to Norvig, a max distance of 1 will cover 80% to 95% of spelling mistakes. After a distance of 2, it becomes very slow.
            
            spellcheck.getCorrections('soemthing', 1); // ['something']
            spellcheck.getCorrections('soemthing', 2); // ['something', 'soothing']
            Development
            
            When developing, please:
            
            Write unit tests
            Make sure your unit tests pass
            The current configuration of the unit tests requires the following environment variable to be set:
            
            export NODE_PATH=.
            License
            
            Copyright (c) 2011, 2012 Chris Umbel, Rob Ellis, Russell Mull
            
            Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
            
            The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
            
            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
            
            WordNet License
            
            This license is available as the file LICENSE in any downloaded version of WordNet. WordNet 3.0 license: (Download)
            
            WordNet Release 3.0 This software and database is being provided to you, the LICENSEE, by Princeton University under the following license. By obtaining, using and/or copying this software and database, you agree that you have read, understood, and will comply with these terms and conditions.: Permission to use, copy, modify and distribute this software and database and its documentation for any purpose and without fee or royalty is hereby granted, provided that you agree to comply with the following copyright notice and statements, including the disclaimer, and that the same appear on ALL copies of the software, database and documentation, including modifications that you make for internal use or for distribution. WordNet 3.0 Copyright 2006 by Princeton University. All rights reserved. THIS SOFTWARE AND DATABASE IS PROVIDED "AS IS" AND PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANT- ABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS. The name of Princeton University or Princeton may not be used in advertising or publicity pertaining to distribution of the software and/or database. Title to copyright in this software, database and any associated documentation shall at all times remain with Princeton University and LICENSEE agrees to preserve same.
        </p>
      </div>
    </section>
    
  </body>
  <script src="basic.js"></script>
</html>
